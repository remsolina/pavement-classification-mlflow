{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6083b1f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6083b1f0",
        "outputId": "f8e2fd05-a2ac-41cf-c4cc-9fe480488ef9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'model_7_s3_class_weighting.ipynb\\n\\nModel 7 from scratch using S3 images, CNN, 256×256 input, and class weighting to address imbalance.\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"model_7_s3_class_weighting.ipynb\n",
        "\n",
        "Model 7 from scratch using S3 images, CNN, 256×256 input, and class weighting to address imbalance.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e365b3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4e365b3e",
        "outputId": "960bd495-64e0-470d-e577-55430ece3d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.37.24-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<1.38.0,>=1.37.24 (from boto3)\n",
            "  Downloading botocore-1.37.24-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.24->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.24->boto3) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.24->boto3) (1.17.0)\n",
            "Downloading boto3-1.37.24-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.24-py3-none-any.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.37.24 botocore-1.37.24 jmespath-1.0.1 s3transfer-0.11.4\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-2.21.2-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting mlflow-skinny==2.21.2 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.21.2-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.6)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.14.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.40)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading databricks_sdk-0.49.0-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting fastapi<1 (from mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (1.31.1)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (2.11.0)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (4.13.0)\n",
            "Collecting uvicorn<1 (from mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (2.38.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.21.2->mlflow)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.21.2->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (1.2.18)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (0.52b1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.2->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.2->mlflow) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.2->mlflow) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.21.2->mlflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.21.2->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.21.2->mlflow) (2025.1.31)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==2.21.2->mlflow) (0.14.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (1.17.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (4.9)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.21.2->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.21.2->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (0.6.1)\n",
            "Downloading mlflow-2.21.2-py3-none-any.whl (28.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.21.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.49.0-py3-none-any.whl (683 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m684.0/684.0 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, gunicorn, graphql-core, starlette, graphql-relay, docker, alembic, graphene, fastapi, databricks-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed alembic-1.15.2 databricks-sdk-0.49.0 docker-7.1.0 fastapi-0.115.12 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.21.2 mlflow-skinny-2.21.2 starlette-0.46.1 uvicorn-0.34.0\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datetime) (2025.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface->datetime) (75.2.0)\n",
            "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-5.5 zope.interface-7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n",
        "!pip install mlflow\n",
        "!pip install datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a7a897",
      "metadata": {
        "id": "80a7a897"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3038d1b1",
      "metadata": {
        "id": "3038d1b1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import boto3\n",
        "import datetime\n",
        "import copy\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9425637",
      "metadata": {
        "id": "f9425637"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# MLflow configuration\n",
        "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'https://s3.us-west-2.amazonaws.com'\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = '*************************'\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = '********************************'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0356d4f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0356d4f2",
        "outputId": "c85df6ee-6d1f-4c27-9c26-3421f1cdbc43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/04/01 14:22:55 WARNING mlflow.utils.autologging_utils: MLflow pytorch autologging is known to be compatible with 1.9.0 <= torch <= 2.6.0, but the installed version is 2.6.0+cu124. If you encounter errors during autologging, try upgrading / downgrading torch to a compatible version, or try upgrading MLflow.\n"
          ]
        }
      ],
      "source": [
        "mlflow.set_tracking_uri(\"http://*******:5000\")  # Replace with your MLflow server URI if different\n",
        "mlflow.set_experiment(\"Pytorch_CNN_from_Scratch_Pavement_Surface_Classification\")\n",
        "mlflow.pytorch.autolog()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae62cbd",
      "metadata": {
        "id": "bae62cbd"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Description for MLflow\n",
        "description = (\n",
        "    \"This version of model_7 loads images from an S3 bucket (instead of local disk) \"\n",
        "    \"and applies class weighting in the loss function to address class imbalance. \"\n",
        "    \"Otherwise, it retains the original CNN architecture (PavementNet) and transformations \"\n",
        "    \"of model_7. It processes grayscale images resized/cropped to 256×256 via data augmentation, \"\n",
        "    \"with a 70/15/15 split, and logs metrics/artifacts to MLflow.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201b70c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "201b70c1",
        "lines_to_next_cell": 1,
        "outputId": "35d09ca0-78d4-4b35-e9aa-1bc9a6749d74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c91b00a4f30>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# For reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a220fc",
      "metadata": {
        "id": "48a220fc",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 1) Define S3 dataset\n",
        "class S3ImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Expects a bucket structure of the form:\n",
        "       s3://<bucket>/<prefix>/<class_name>/image.jpg\n",
        "    \"\"\"\n",
        "    def __init__(self, bucket_name, prefix, transform=None):\n",
        "        super().__init__()\n",
        "        self.s3 = boto3.client(\"s3\")\n",
        "        self.bucket_name = bucket_name\n",
        "        # Ensure prefix has no trailing slash\n",
        "        self.prefix = prefix.rstrip(\"/\")\n",
        "        self.transform = transform\n",
        "\n",
        "        self.samples = []\n",
        "        self.classes = set()\n",
        "\n",
        "        # List all objects in S3 under the given prefix\n",
        "        paginator = self.s3.get_paginator(\"list_objects_v2\")\n",
        "        pages = paginator.paginate(Bucket=self.bucket_name, Prefix=self.prefix)\n",
        "\n",
        "        for page in pages:\n",
        "            if \"Contents\" in page:\n",
        "                for obj in page[\"Contents\"]:\n",
        "                    key = obj[\"Key\"]\n",
        "                    # Check if this key points to an image\n",
        "                    if key.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\")):\n",
        "                        # Typically the structure is prefix/class_name/image_file\n",
        "                        parts = key.split(\"/\")\n",
        "                        if len(parts) >= 2:\n",
        "                            class_name = parts[-2]\n",
        "                            self.samples.append((key, class_name))\n",
        "                            self.classes.add(class_name)\n",
        "\n",
        "        self.classes = sorted(list(self.classes))\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        # Convert (key, class_name) => (key, class_index)\n",
        "        self.samples = [(k, self.class_to_idx[c]) for (k, c) in self.samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s3_key, label = self.samples[idx]\n",
        "        s3_obj = self.s3.get_object(Bucket=self.bucket_name, Key=s3_key)\n",
        "        image_bytes = s3_obj[\"Body\"].read()\n",
        "        image = Image.open(BytesIO(image_bytes))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588aac39",
      "metadata": {
        "id": "588aac39"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# 2) Define the original transforms from model_7\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda img: img.convert(\"L\")),  # Force grayscale\n",
        "    transforms.Resize((280, 280)),                    # Resize to 280×280\n",
        "    transforms.RandomCrop(256),                       # Random crop to 256×256\n",
        "    transforms.RandomHorizontalFlip(),                # Random horizontal flip\n",
        "    transforms.RandomRotation(10),                    # Random rotation\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),                            # (1, 256, 256)\n",
        "    transforms.Normalize((0.5,), (0.5,))              # Normalize\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af842fc4",
      "metadata": {
        "id": "af842fc4"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda img: img.convert(\"L\")),  # Force grayscale\n",
        "    transforms.Resize((256, 256)),                    # Resize to 256×256\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5959683",
      "metadata": {
        "id": "d5959683"
      },
      "outputs": [],
      "source": [
        "# --------------------------------\n",
        "# 3) Load the full dataset WITHOUT transform first, so we can split\n",
        "S3_BUCKET_NAME = \"myfinaldata\"  # <--- REPLACE with your bucket\n",
        "S3_PREFIX      = \"finaldata\"    # <--- REPLACE with your prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ca16ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1ca16ee",
        "outputId": "865f0091-c6db-4850-f7be-de528ab0514d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes found in S3: ['asphalt', 'chip-sealed', 'gravel']\n",
            "Total images found: 6500\n"
          ]
        }
      ],
      "source": [
        "full_dataset_no_transform = S3ImageDataset(\n",
        "    bucket_name=S3_BUCKET_NAME,\n",
        "    prefix=S3_PREFIX,\n",
        "    transform=None\n",
        ")\n",
        "print(\"Classes found in S3:\", full_dataset_no_transform.classes)\n",
        "print(\"Total images found:\", len(full_dataset_no_transform))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce84792e",
      "metadata": {
        "id": "ce84792e"
      },
      "outputs": [],
      "source": [
        "# --------------------------------\n",
        "# 4) 70/15/15 split across classes (stratified)\n",
        "label_to_indices = defaultdict(list)\n",
        "for idx, (key, lbl_idx) in enumerate(full_dataset_no_transform.samples):\n",
        "    label_to_indices[lbl_idx].append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7384c6f",
      "metadata": {
        "id": "f7384c6f"
      },
      "outputs": [],
      "source": [
        "train_indices = []\n",
        "val_indices = []\n",
        "test_indices = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb5b1e54",
      "metadata": {
        "id": "eb5b1e54"
      },
      "outputs": [],
      "source": [
        "for lbl, indices in label_to_indices.items():\n",
        "    random.shuffle(indices)\n",
        "    n = len(indices)\n",
        "    train_count = int(0.70 * n)\n",
        "    val_count   = int(0.15 * n)\n",
        "    # remainder -> test\n",
        "    test_count  = n - train_count - val_count\n",
        "\n",
        "    train_indices.extend(indices[:train_count])\n",
        "    val_indices.extend(indices[train_count:train_count + val_count])\n",
        "    test_indices.extend(indices[train_count + val_count:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef8f71ee",
      "metadata": {
        "id": "ef8f71ee",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "random.shuffle(train_indices)\n",
        "random.shuffle(val_indices)\n",
        "random.shuffle(test_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b2597a",
      "metadata": {
        "id": "d9b2597a",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def print_class_distribution(indices, dataset, subset_name):\n",
        "    from collections import Counter\n",
        "    labels = [dataset.samples[i][1] for i in indices]\n",
        "    distribution = Counter(labels)\n",
        "    print(f\"{subset_name} distribution:\")\n",
        "    for label, count in distribution.items():\n",
        "        cls_name = dataset.classes[label]\n",
        "        print(f\"  {cls_name}: {count}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64dbb1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c64dbb1a",
        "lines_to_next_cell": 1,
        "outputId": "79583d31-72d9-4fe5-903b-b9d670d251ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Set distribution:\n",
            "  asphalt: 3500\n",
            "  chip-sealed: 700\n",
            "  gravel: 350\n",
            "\n",
            "Validation Set distribution:\n",
            "  asphalt: 750\n",
            "  chip-sealed: 150\n",
            "  gravel: 75\n",
            "\n",
            "Test Set distribution:\n",
            "  asphalt: 750\n",
            "  chip-sealed: 150\n",
            "  gravel: 75\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_class_distribution(train_indices, full_dataset_no_transform, \"Train Set\")\n",
        "print_class_distribution(val_indices,   full_dataset_no_transform, \"Validation Set\")\n",
        "print_class_distribution(test_indices,  full_dataset_no_transform, \"Test Set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc51aefd",
      "metadata": {
        "id": "bc51aefd",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# --------------------------------\n",
        "# 5) Create subsets with appropriate transforms\n",
        "def create_subset(dataset, indices, transform):\n",
        "    # We'll create a copy that references only the subset's samples but with the new transform\n",
        "    subset_ds = S3ImageDataset(dataset.bucket_name, dataset.prefix, transform=transform)\n",
        "    subset_ds.classes       = dataset.classes\n",
        "    subset_ds.class_to_idx  = dataset.class_to_idx\n",
        "    # Filter only these indices\n",
        "    subset_ds.samples       = [dataset.samples[i] for i in indices]\n",
        "    return subset_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5daf1ab3",
      "metadata": {
        "id": "5daf1ab3"
      },
      "outputs": [],
      "source": [
        "train_dataset = create_subset(full_dataset_no_transform, train_indices, train_transform)\n",
        "val_dataset   = create_subset(full_dataset_no_transform, val_indices,   test_transform)\n",
        "test_dataset  = create_subset(full_dataset_no_transform, test_indices,  test_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3acbcb13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3acbcb13",
        "outputId": "00a8e454-73de-416e-f245-cde8b13473ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 4550\n",
            "Val dataset size:   975\n",
            "Test dataset size:  975\n"
          ]
        }
      ],
      "source": [
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Val dataset size:  \", len(val_dataset))\n",
        "print(\"Test dataset size: \", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05dceca4",
      "metadata": {
        "id": "05dceca4"
      },
      "outputs": [],
      "source": [
        "# --------------------------------\n",
        "# 6) Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94e7f3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d94e7f3a",
        "outputId": "599fc6e5-3f01-495b-d2a8-b92cfd9bfe0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample batch - images.shape: torch.Size([32, 1, 256, 256]) labels.shape: torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# Quick shape check (comment out if not needed)\n",
        "for images, labels in train_loader:\n",
        "    print(\"Sample batch - images.shape:\", images.shape, \"labels.shape:\", labels.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60dccbf5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60dccbf5",
        "outputId": "61880ec8-0b20-4c92-ff10-86bd233cf24e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class sample counts (train subset): [3500  700  350]\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------\n",
        "# 7) Compute class weights from training set to address imbalance\n",
        "#    We'll do inverse-frequency weighting: weight ~ 1/freq.\n",
        "train_labels = [full_dataset_no_transform.samples[i][1] for i in train_indices]\n",
        "class_counts = np.bincount(train_labels)\n",
        "print(\"Class sample counts (train subset):\", class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d910a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5d910a1",
        "lines_to_next_cell": 1,
        "outputId": "5e2a5e31-2ca0-4fc1-e000-2410310f885c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights (inverse-freq): tensor([0.0625, 0.3125, 0.6250])\n"
          ]
        }
      ],
      "source": [
        "# Inverse frequency\n",
        "weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
        "weights = weights / weights.sum()  # optional normalization\n",
        "print(\"Class weights (inverse-freq):\", weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92aa5d4b",
      "metadata": {
        "id": "92aa5d4b",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# --------------------------------\n",
        "# 8) Define the original CNN architecture from model_7\n",
        "class PavementNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PavementNet, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)   # Input: (1,256,256) -> (32,256,256)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # (64,256,256)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # (128,256,256)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # After 3 x (conv+pool), 256 -> 128 -> 64 -> 32 in H/W\n",
        "        # Then adaptive average pool to (8,8)\n",
        "        self.adapt_pool = nn.AdaptiveAvgPool2d((8, 8))\n",
        "\n",
        "        # Fully connected layers\n",
        "        # Flatten => 128 * 8 * 8 = 8192\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 3)  # We have 3 classes (asphalt, chip-sealed, gravel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.adapt_pool(x)    # (128,8,8)\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)           # logits\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc28d6d",
      "metadata": {
        "id": "cbc28d6d"
      },
      "outputs": [],
      "source": [
        "# --------------------------------\n",
        "# 9) Training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PavementNet().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2c3d0b9",
      "metadata": {
        "id": "b2c3d0b9"
      },
      "outputs": [],
      "source": [
        "# Use weighted cross-entropy\n",
        "criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "693f089d",
      "metadata": {
        "id": "693f089d"
      },
      "outputs": [],
      "source": [
        "num_epochs = 30\n",
        "train_losses = []\n",
        "val_losses   = []\n",
        "train_accuracies = []\n",
        "val_accuracies   = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c8697ba",
      "metadata": {
        "id": "3c8697ba"
      },
      "outputs": [],
      "source": [
        "# A distinctive run name\n",
        "run_name = f\"model_7_s3_cnn_grayscale_classweight_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c94dccc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c94dccc",
        "outputId": "8d06f93f-a885-4f72-8aa7-8fdd524cbb81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30  Train Loss: 0.9940  Train Acc: 0.6051  Val Loss: 0.8714  Val Acc: 0.1815\n",
            "Epoch 2/30  Train Loss: 0.8151  Train Acc: 0.6934  Val Loss: 0.5436  Val Acc: 0.8831\n",
            "Epoch 3/30  Train Loss: 0.5453  Train Acc: 0.8224  Val Loss: 0.7703  Val Acc: 0.8769\n",
            "Epoch 4/30  Train Loss: 0.4100  Train Acc: 0.8809  Val Loss: 0.3163  Val Acc: 0.9138\n"
          ]
        }
      ],
      "source": [
        "with mlflow.start_run(run_name=run_name):\n",
        "    # Log a description and hyperparams\n",
        "    mlflow.set_tag(\"description\", description)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"batch_size\", batch_size)\n",
        "    mlflow.log_param(\"learning_rate\", 0.001)\n",
        "    mlflow.log_param(\"input_size\", (256, 256))\n",
        "    mlflow.log_param(\"architecture\", \"PavementNet (3xConv->Pool + AdaptivePool->FC)\")\n",
        "    mlflow.log_param(\"s3_bucket\", S3_BUCKET_NAME)\n",
        "    mlflow.log_param(\"s3_prefix\", S3_PREFIX)\n",
        "    mlflow.log_param(\"class_weighting\", \"inverse_frequency\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ------------- TRAIN -------------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_train += (preds == labels).sum().item()\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_dataset)\n",
        "        epoch_train_acc  = correct_train / len(train_dataset)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accuracies.append(epoch_train_acc)\n",
        "\n",
        "        # ------------- VALIDATION -------------\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item() * images.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct_val += (preds == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_dataset)\n",
        "        epoch_val_acc  = correct_val / len(val_dataset)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(epoch_val_acc)\n",
        "\n",
        "        # Logging\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}  \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}  Train Acc: {epoch_train_acc:.4f}  \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}  Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "        mlflow.log_metric(\"train_loss\", epoch_train_loss, step=epoch)\n",
        "        mlflow.log_metric(\"train_accuracy\", epoch_train_acc, step=epoch)\n",
        "        mlflow.log_metric(\"val_loss\", epoch_val_loss, step=epoch)\n",
        "        mlflow.log_metric(\"val_accuracy\", epoch_val_acc, step=epoch)\n",
        "\n",
        "    # --------------------------------\n",
        "    # Plot & log training curves\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
        "    plt.plot(range(1, num_epochs + 1), val_losses, label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "    loss_curve_path = \"loss_curve.png\"\n",
        "    plt.savefig(loss_curve_path)\n",
        "    mlflow.log_artifact(loss_curve_path)\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, num_epochs + 1), train_accuracies, label=\"Train Accuracy\")\n",
        "    plt.plot(range(1, num_epochs + 1), val_accuracies, label=\"Val Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training and Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    acc_curve_path = \"accuracy_curve.png\"\n",
        "    plt.savefig(acc_curve_path)\n",
        "    mlflow.log_artifact(acc_curve_path)\n",
        "    plt.close()\n",
        "\n",
        "    # --------------------------------\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    # Classification report and confusion matrix\n",
        "    class_names = full_dataset_no_transform.classes\n",
        "    class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
        "    print(\"Classification Report:\\n\", class_report)\n",
        "\n",
        "    report_dict = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
        "    mlflow.log_metric(\"test_accuracy\", report_dict[\"accuracy\"])\n",
        "\n",
        "    # Per-class metrics\n",
        "    for cls, metrics in report_dict.items():\n",
        "        if isinstance(metrics, dict):\n",
        "            mlflow.log_metric(f\"{cls}_precision\", metrics.get(\"precision\", 0))\n",
        "            mlflow.log_metric(f\"{cls}_recall\", metrics.get(\"recall\", 0))\n",
        "            mlflow.log_metric(f\"{cls}_f1-score\", metrics.get(\"f1-score\", 0))\n",
        "\n",
        "    # Save & log classification report\n",
        "    report_path = \"classification_report.txt\"\n",
        "    with open(report_path, \"w\") as f:\n",
        "        f.write(class_report)\n",
        "    mlflow.log_artifact(report_path)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    cm_path = \"confusion_matrix.png\"\n",
        "    plt.savefig(cm_path)\n",
        "    mlflow.log_artifact(cm_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Log the final trained model to MLflow\n",
        "    mlflow.pytorch.log_model(model, \"model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
